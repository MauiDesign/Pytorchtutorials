{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x116e10ad0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Creating Tensors\n",
    "# ~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# Tensors can be created from Python lists with the torch.tensor()\n",
    "# function.\n",
    "#\n",
    "\n",
    "# torch.tensor(data) creates a torch.Tensor object with the given data.\n",
    "V_data = [1., 2., 3.]\n",
    "V = torch.tensor(V_data)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# Creates a matrix\n",
    "M_data = [[1., 2., 3.], [4., 5., 6]]\n",
    "M = torch.tensor(M_data)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "# Create a 3D tensor of size 2x2x2.\n",
    "T_data = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "T = torch.tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Index into V and get a scalar (0 dimensional tensor)\n",
    "print(V[0])\n",
    "# Get a Python number from it\n",
    "print(V[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Index into M and get a vector\n",
    "print(M[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# Index into T and get a matrix\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1852,  0.3221, -1.4649,  0.1153,  1.1154],\n",
      "         [-2.3996, -0.2961, -0.4604,  0.1779,  0.7929],\n",
      "         [-0.1433,  0.6640,  1.2030,  1.2682,  0.6134],\n",
      "         [-0.8820,  1.0452,  0.6023,  0.1903,  0.1112]],\n",
      "\n",
      "        [[ 0.7686,  0.1050,  1.2035, -0.4381, -1.2662],\n",
      "         [ 1.5489, -0.0979,  1.8058,  0.8630, -0.9722],\n",
      "         [ 1.4394, -0.5560, -0.2039,  0.1190,  0.2423],\n",
      "         [ 0.5120,  0.7333,  1.4497,  0.4509,  0.9503]],\n",
      "\n",
      "        [[-0.1641, -0.4650, -0.3298, -1.0040, -0.9307],\n",
      "         [-0.7649,  0.6117, -0.3254,  1.9492,  0.7256],\n",
      "         [-0.2675,  0.5285,  0.2760,  0.8485, -0.0779],\n",
      "         [-0.1363, -0.1086, -1.1616,  0.5524,  0.1481]]])\n"
     ]
    }
   ],
   "source": [
    "# You can also create tensors of other data types. To create a tensor of integer types, try\n",
    "# torch.tensor([[1, 2], [3, 4]]) (where all elements in the list are integers).\n",
    "# You can also specify a data type by passing in ``dtype=torch.data_type``.\n",
    "# Check the documentation for more data types, but\n",
    "# Float and Long will be the most common.\n",
    "\n",
    "# You can create a tensor with random data and the supplied dimensionality\n",
    "# with torch.randn()\n",
    "#\n",
    "\n",
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "# Operations with Tensors\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# You can operate on tensors in the ways you would expect.\n",
    "\n",
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5800, -0.2400,  1.1887,  0.5137, -0.2237],\n",
      "        [-2.4069,  0.3051, -1.7837, -1.6008,  0.2391],\n",
      "        [ 0.4831, -1.7941,  1.0521,  0.4921,  0.3959],\n",
      "        [-0.8886, -0.1213, -0.3444,  1.1246, -0.8451],\n",
      "        [ 1.6779,  0.2403, -0.8946,  0.9642,  0.0042]])\n"
     ]
    }
   ],
   "source": [
    "# See `the documentation <https://pytorch.org/docs/torch.html>`__ for a\n",
    "# complete list of the massive number of operations available to you. They\n",
    "# expand beyond just mathematical operations.\n",
    "#\n",
    "# One helpful operation that we will make use of later is concatenation.\n",
    "\n",
    "\n",
    "# By default, it concatenates along the first axis (concatenates rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0060,  0.7094,  0.7479, -1.0113,  1.1192,  0.3528, -0.3043, -0.3187],\n",
      "        [-1.7720,  0.0824, -1.2363, -0.8834,  0.7879,  0.5873, -1.9456, -0.4122]])\n"
     ]
    }
   ],
   "source": [
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)\n",
    "\n",
    "# If your tensors are not compatible, torch will complain.  Uncomment to see the error\n",
    "# torch.cat([x_1, x_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0972,  0.5737,  1.1018, -0.3262],\n",
      "         [ 1.4643,  0.9163,  0.4538, -1.5773],\n",
      "         [ 0.1156, -0.0415,  0.2913,  0.2278]],\n",
      "\n",
      "        [[-0.3871, -0.4712, -0.2860, -0.2035],\n",
      "         [-0.9943,  1.7137,  0.9367, -1.7576],\n",
      "         [-0.3883, -0.0700, -0.2071,  1.2493]]])\n",
      "tensor([[ 0.0972,  0.5737,  1.1018, -0.3262,  1.4643,  0.9163,  0.4538, -1.5773,\n",
      "          0.1156, -0.0415,  0.2913,  0.2278],\n",
      "        [-0.3871, -0.4712, -0.2860, -0.2035, -0.9943,  1.7137,  0.9367, -1.7576,\n",
      "         -0.3883, -0.0700, -0.2071,  1.2493]])\n"
     ]
    }
   ],
   "source": [
    "# Reshaping Tensors\n",
    "# ~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# Use the .view() method to reshape a tensor. This method receives heavy\n",
    "# use, because many neural network components expect their inputs to have\n",
    "# a certain shape. Often you will need to reshape before passing your data\n",
    "# to the component.\n",
    "#\n",
    "\n",
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0972,  0.5737,  1.1018, -0.3262,  1.4643,  0.9163,  0.4538, -1.5773,\n",
      "          0.1156, -0.0415,  0.2913,  0.2278],\n",
      "        [-0.3871, -0.4712, -0.2860, -0.2035, -0.9943,  1.7137,  0.9367, -1.7576,\n",
      "         -0.3883, -0.0700, -0.2071,  1.2493]])\n"
     ]
    }
   ],
   "source": [
    "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation Graphs and Automatic Differentiation\n",
    "# ================================================\n",
    "#\n",
    "# The concept of a computation graph is essential to efficient deep\n",
    "# learning programming, because it allows you to not have to write the\n",
    "# back propagation gradients yourself. A computation graph is simply a\n",
    "# specification of how your data is combined to give you the output. Since\n",
    "# the graph totally specifies what parameters were involved with which\n",
    "# operations, it contains enough information to compute derivatives. This\n",
    "# probably sounds vague, so let's see what is going on using the\n",
    "# fundamental flag ``requires_grad``.\n",
    "#\n",
    "# First, think from a programmers perspective. What is stored in the\n",
    "# torch.Tensor objects we were creating above? Obviously the data and the\n",
    "# shape, and maybe a few other things. But when we added two tensors\n",
    "# together, we got an output tensor. All this output tensor knows is its\n",
    "# data and shape. It has no idea that it was the sum of two other tensors\n",
    "# (it could have been read in from a file, it could be the result of some\n",
    "# other operation, etc.)\n",
    "#\n",
    "# If ``requires_grad=True``, the Tensor object keeps track of how it was\n",
    "# created. Lets see it in action.\n",
    "#\n",
    "\n",
    "# Tensor factory methods have a ``requires_grad`` flag\n",
    "x = torch.tensor([1., 2., 3], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# With requires_grad=True, you can still do all the operations you previously\n",
    "# could\n",
    "y = torch.tensor([4., 5., 6], requires_grad=True)\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x11aa14d10>\n"
     ]
    }
   ],
   "source": [
    "# BUT z knows something extra.\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21., grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x11aa14ed0>\n"
     ]
    }
   ],
   "source": [
    "# So Tensors know what created them. z knows that it wasn't read in from\n",
    "# a file, it wasn't the result of a multiplication or exponential or\n",
    "# whatever. And if you keep following z.grad_fn, you will find yourself at\n",
    "# x and y.\n",
    "#\n",
    "# But how does that help us compute a gradient?\n",
    "#\n",
    "\n",
    "# Lets sum up all the entries in z\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# So now, what is the derivative of this sum with respect to the first\n",
    "# component of x? In math, we want\n",
    "#\n",
    "# .. math::\n",
    "#\n",
    "#    \\frac{\\partial s}{\\partial x_0}\n",
    "#\n",
    "#\n",
    "#\n",
    "# Well, s knows that it was created as a sum of the tensor z. z knows\n",
    "# that it was the sum x + y. So\n",
    "#\n",
    "# .. math::  s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}\n",
    "#\n",
    "# And so s contains enough information to determine that the derivative\n",
    "# we want is 1!\n",
    "#\n",
    "# Of course this glosses over the challenge of how to actually compute\n",
    "# that derivative. The point here is that s is carrying along enough\n",
    "# information that it is possible to compute it. In reality, the\n",
    "# developers of Pytorch program the sum() and + operations to know how to\n",
    "# compute their gradients, and run the back propagation algorithm. An\n",
    "# in-depth discussion of that algorithm is beyond the scope of this\n",
    "# tutorial.\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Lets have Pytorch compute the gradient, and see that we were right:\n",
    "# (note if you run this block multiple times, the gradient will increment.\n",
    "# That is because Pytorch *accumulates* the gradient into the .grad\n",
    "# property, since for many models this is very convenient.)\n",
    "#\n",
    "\n",
    "# calling .backward() on any variable will run backprop, starting from it.\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n"
     ]
    }
   ],
   "source": [
    "# Understanding what is going on in the block below is crucial for being a\n",
    "# successful programmer in deep learning.\n",
    "#\n",
    "\n",
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)\n",
    "# By default, user created Tensors have ``requires_grad=False``\n",
    "print(x.requires_grad, y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "z = x + y\n",
    "# So you can't backprop through z\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x11aa24690>\n"
     ]
    }
   ],
   "source": [
    "# ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n",
    "# flag in-place. The input flag defaults to ``True`` if not given.\n",
    "x = x.requires_grad_()\n",
    "y = y.requires_grad_()\n",
    "# z contains enough information to compute gradients, as we saw above\n",
    "z = x + y\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# If any input to an operation has ``requires_grad=True``, so will the output\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now z has the computation history that relates itself to x and y\n",
    "# Can we just take its values, and **detach** it from its history?\n",
    "new_z = z.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# ... does new_z have information to backprop to x and y?\n",
    "# NO!\n",
    "\n",
    "print(new_z.grad_fn)\n",
    "\n",
    "# And how could it? ``z.detach()`` returns a tensor that shares the same storage\n",
    "# as ``z``, but with the computation history forgotten. It doesn't know anything\n",
    "# about how it was computed.\n",
    "# In essence, we have broken the Tensor away from its past history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# You can also stop autograd from tracking history on Tensors\n",
    "# with ``.requires_grad``=True by wrapping the code block in\n",
    "# ``with torch.no_grad():``\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\tprint((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
